{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suhyeonlee0827/2023-2-AI-Study/blob/main/Transformer_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFFmt1G9yMM_"
      },
      "source": [
        "# NLP 2 과제\n",
        "> 인공지능 스터디 일곱 번째 과제에 오신 것을 환영합니다! 강의를 들으면서 배운 다양한 지식들을 실습을 통해서 활용해 볼 시간을 가질 것입니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwsLJnEXyMND"
      },
      "source": [
        "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Transformer\n",
        "\n",
        "아래의 수식과 같이 계산되는 multi-head attention에서 query, key, value 벡터를 생성하기 위한\n",
        "projection matrix $( W_{i}^{Q}, W_{i}^{K}, W_{i}^{V}​​ )$는 head 간에 sharing 된다. <br>\n",
        "***\n",
        "$ MultiHead(Q,K,V)=Concat(head_1, \\cdots, head_h)W^{O} $ (이때, $W^{O}$ 는 Output을 만들때 사용되는 가중치 행렬)<br>\n",
        "where $head_i=Attention(QW^{Q}_{i}, KW^{K}_{i}, VW^{V}_{i})$ (이때, $Q, K, V$ 는 입력에서 tokenize된 단어들의 임베딩 벡터 $Q = K = V$ )\n",
        "```python\n",
        "(1) 예\n",
        "(2) 아니오\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cYXp3CLyMNE"
      },
      "source": [
        "```python\n",
        "😉\n",
        "# TODO : 정답을 적어주세요\n",
        "1\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNU3T0KdyMNE"
      },
      "source": [
        "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Transformer\n",
        "```python\n",
        "Transformer 모델에서 각 입력 토큰들이 가진 순서를 입력하기 위해 사용하는 방법을 고르시오.\n",
        "\n",
        "(1) Positional Encoding\n",
        "(2) Encoder-Decoder attention\n",
        "(3) Layer normalization\n",
        "(4) Masked decoder self-attention\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzebY-8lyMNF"
      },
      "source": [
        "```python\n",
        "😉\n",
        "# TODO : 정답을 적어주세요\n",
        "1\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp43YODayMNF"
      },
      "source": [
        "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> GPT\n",
        "```python\n",
        "GPT-1 모델이 어떻게 다양한 자연어 처리 태스크에서 사용될 수 있는지 설명해주세요.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-r_nu_YyMNF"
      },
      "source": [
        "```python\n",
        "😉\n",
        "# TODO : 정답을 적어주세요\n",
        "Supervised fine-tuning took as few as 3 epochs for most of the downstream tasks.\n",
        "GPT-1 performed better than specifically trained supervised state-of-the-art models in 9 out of 12 tasks the models were compared on.\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn7YgSGhyMNF"
      },
      "source": [
        "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> GPT\n",
        "```python\n",
        "GPT-1 모델의 \"GPT\" 약자는 무엇을 의미하나요?\n",
        "\n",
        "(1) Generalized Pre-trained Transformer\n",
        "(2) Generative Pre-trained Transformer\n",
        "(3) Globalized Pre-processing Transformer\n",
        "(4) Gradient Propagation Technique\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr-F_vW_yMNG"
      },
      "source": [
        "```python\n",
        "😉\n",
        "# TODO : 정답을 적어주세요\n",
        "2\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gk-b7hIyMNG"
      },
      "source": [
        "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> BERT\n",
        "```python\n",
        "다음 중 BERT에 대한 설명으로 옳지 않은 것을 고르시오.\n",
        "\n",
        "(1) 학습 데이터에서 [MASK] 토큰이 선택되는 비율이 극단적으로 작은 경우, 모델 학습을 위한 비용이 증가한다.\n",
        "(2) Unidirectional model로 자연어 생성에 특화된 모델이다.\n",
        "(3) 입력 시퀀스 중 일부 마스킹된 토큰을 맞추는 masked language modeling (masked LM)을 통해 pre-training을 수행하였다.\n",
        "(4) 사전학습을 위한 [MASK] 토큰은 random하게 선택된다.\n",
        "(5) Unlabeled 데이터를 기반으로 self-supervised learning을 적용하여 사전학습한 모델이다.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJa3t1UDyMNG"
      },
      "source": [
        "```python\n",
        "😉\n",
        "# TODO : 정답을 적어주세요\n",
        "2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJUi06fwyMNG"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 실습 ]</b></font> Multi-head Attention\n",
        "```python\n",
        "이번 실습을 통해 다음 2가지를 알아볼 것입니다.\n",
        "1. Multi-head attention 및 self-attention을 구현합니다.\n",
        "2. 각 과정에서 일어나는 연산과 input/output 형태를 이해합니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulhN-MGmyMNG"
      },
      "source": [
        "```python\n",
        "🐙\n",
        "먼저 코드 실행에 필요한 패키지를 import 해봅시다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQXUzuoGyMNH"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_3ps0DzyMNI"
      },
      "source": [
        "### 데이터 전처리\n",
        "```python\n",
        "저번 주차의 데이터와 비슷한 형태입니다.\n",
        "먼저 전체 단어 수인 vocab_size가 주어집니다.\n",
        "pad_id는 주어진 데이터의 길이를 맞춰주기 위해 패딩을 진행하게 되는데 이때 패딩을 의미하는 토큰의 id입니다.\n",
        "sample data 보면 숫자로 이루어진 것을 볼 수 있는데 이는 저희가 구성한 vocab에서 몇 번째 단어인지를 의미합니다.\n",
        "따라서 데이터의 각 요소를 단어로 이루어진 문장이라고 생각할 수 있습니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPNB5JgJyMNI"
      },
      "outputs": [],
      "source": [
        "vocab_size = 100\n",
        "pad_id = 0\n",
        "\n",
        "data = [\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
        "  [60, 96, 51, 32, 90],\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
        "  [75, 51],\n",
        "  [66, 88, 98, 47],\n",
        "  [21, 39, 10, 64, 21],\n",
        "  [98],\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFZn-yb6yMNI"
      },
      "source": [
        "```python\n",
        "주어진 데이터의 길이를 맞춰주기 위한 padding 함수를 도입합니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87n2VzFSyMNI"
      },
      "outputs": [],
      "source": [
        "def padding(data):\n",
        "  max_len = len(max(data, key=len))\n",
        "  print(f\"Maximum sequence length: {max_len}\")\n",
        "\n",
        "  for i, seq in enumerate(tqdm(data)):\n",
        "    if len(seq) < max_len:\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
        "\n",
        "  return data, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBGPI6wayMNI"
      },
      "outputs": [],
      "source": [
        "data, max_len = padding(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42SF3HZkyMNI"
      },
      "source": [
        "```python\n",
        "전처리된 데이터를 확인해 보면 잘 패딩 되었음을 확인할 수 있습니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M82JFBmVyMNI"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHosi9payMNJ"
      },
      "source": [
        "### Hyperparameter 세팅 및 embedding\n",
        "```python\n",
        "위 데이터를 임베딩하여 실습에 사용할 데이터를 만들어 봅시다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpQlKlmIyMNJ"
      },
      "outputs": [],
      "source": [
        "d_model = 512  # model의 hidden size\n",
        "num_heads = 8  # multi-head에서의 head의 개수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwkPDG7pyMNJ"
      },
      "outputs": [],
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "# B: 배치 사이즈, L: maximum sequence length\n",
        "batch = torch.LongTensor(data)  # (B, L)\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4mOsNAryMNJ"
      },
      "outputs": [],
      "source": [
        "print(batch_emb)\n",
        "print(batch_emb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBeaEK_eyMNJ"
      },
      "source": [
        "### Linear transformation & 여러 head로 나누기\n",
        "```python\n",
        "Multi-head attention 내에서 쓰이는 linear transformation matrix들을 정의합니다.\n",
        "\n",
        "query, key, value를 서로 다른 linear transformation matrix로 행렬 연산을 통해 만들어 냅니다. 따라서 동일한 데이터(batch_emb)로부터 서로 다른 query, key, value를 생성할 수 있습니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZorbczD8yMNJ"
      },
      "outputs": [],
      "source": [
        "w_q = nn.Linear(d_model, d_model)\n",
        "w_k = nn.Linear(d_model, d_model)\n",
        "w_v = nn.Linear(d_model, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eU64c_IyMNJ"
      },
      "source": [
        "```python\n",
        "output layer에서 사용될 행렬도 만들어 줍니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jv8oAH8nyMNK"
      },
      "outputs": [],
      "source": [
        "w_0 = nn.Linear(d_model, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1boaN8myMNK"
      },
      "outputs": [],
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cP5KLeQyMNK"
      },
      "source": [
        "```python\n",
        "q, k, v를 'num_head' 개의 차원으로 분할하여 여러 벡터를 만듭니다.\n",
        "실제 q, k, v 각각의 벡터 크기는 512가 아닌 64입니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38NCX05ryMNK"
      },
      "outputs": [],
      "source": [
        "batch_size = q.shape[0]\n",
        "d_k = d_model // num_heads # q, k, v 벡터 사이즈\n",
        "\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ejx-WARSyMNK"
      },
      "source": [
        "```python\n",
        "8개의 head에 필요한 q, k, v가 만들어졌습니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjBEMC0yyMNK"
      },
      "outputs": [],
      "source": [
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kU6r-yCyMNK"
      },
      "source": [
        "### Scaled dot-product self-attention 구현\n",
        "```python\n",
        "각 head에서 실행되는 self-attention 과정을 살펴봅시다.\n",
        "\n",
        "q, k 벡터의 내적 연산 이후에 d_k의 제곱근으로 나눠줍니다.\n",
        "이는 q와 k를 구성하는 요소의 평균과 분산을 내적의 결괏값에 대해서도 유지시켜주기 위함입니다.\n",
        "\n",
        "이후 계산된 각 행에 대해서 softmax 연산을 통해서 각 요소의 합을 1로 만들어줍니다.\n",
        "```\n",
        "- [Scaled Dot-Product Attention 참고](https://paperswithcode.com/method/scaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6cT2xHqyMNL"
      },
      "outputs": [],
      "source": [
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "print(attn_dists)\n",
        "print(attn_dists.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXkizw1fyMNL"
      },
      "source": [
        "```python\n",
        "이후 계산된 attention 값을 v과 곱하여 최종 결괏값을 제시합니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stLnfAmwyMNL"
      },
      "outputs": [],
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(attn_values.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruTnpzmdyMNL"
      },
      "source": [
        "### 각 head의 결과 병합(concat)\n",
        "```python\n",
        "각 head의 결과물을 concat하고 동일 차원(d_model)으로 linear transformation 합니다.\n",
        "\n",
        "여기서 'd_model' 차원으로 linear transformation 하는 이유는 transformer 모델에서 원래의 데이터와 더하는 연산(residual connection)이 존재하여 이때 차원을 통일해야 하기 때문입니다.\n",
        "\n",
        "residual connection 연산은 아래 이미지에서 Self-Attention 블록 이후 Add에 해당하는 연산입니다.\n",
        "\n",
        "residual connection은 앞선 강의에서 배운 resnet에서 소개된 기술입니다.\n",
        "```\n",
        "![residual](https://github.com/Pjunn/GDSC_mlstudy/blob/main/7%EC%A3%BC%EC%B0%A8/transformer_resideual_layer_norm.png?raw=true)\n",
        "이미지 출처: https://jalammar.github.io/illustrated-transformer/ <br><br>\n",
        "-[What is Residual Connection?](https://paperswithcode.com/method/residual-connection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IK_T826EyMNQ"
      },
      "outputs": [],
      "source": [
        "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
        "\n",
        "print(attn_values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTtl73-RyMNQ"
      },
      "outputs": [],
      "source": [
        "outputs = w_0(attn_values)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHXDWz_ryMNQ"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈을 구현해 봅시다.\n",
        "```python\n",
        "🐙\n",
        "아래의 Multi-head attention 모듈에서 '#TODO'를 채워 모듈을 완성 시켜주세요.\n",
        "위 실습에서 배운 내용이 큰 힌트가 될 거예요!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_k6hn7tyMNQ"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, dim_model, num_heads):\n",
        "    super(MultiheadAttention, self).__init__()\n",
        "\n",
        "    assert dim_model % num_heads == 0\n",
        "\n",
        "    self.dim_model = dim_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = #TODO\n",
        "\n",
        "    # Q, K, V 변환시켜주는 레이어\n",
        "    self.w_q = #TODO\n",
        "    self.w_k = #TODO\n",
        "    self.w_v = #TODO\n",
        "\n",
        "    # concat된 아웃풋을 변환시켜주는 레이어\n",
        "    self.w_0 = #TODO\n",
        "\n",
        "  def forward(self, query, key, value):\n",
        "\n",
        "    #TODO\n",
        "\n",
        "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    #TODO\n",
        "\n",
        "    output =\n",
        "\n",
        "    return outputs\n",
        "\n",
        "  def self_attention(self, q, k, v):\n",
        "    #TODO\n",
        "\n",
        "    return attn_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewyDufC9yMNR"
      },
      "outputs": [],
      "source": [
        "dim_model = 512\n",
        "num_heads = 8\n",
        "multihead_attn = MultiheadAttention(dim_model, num_heads)\n",
        "\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HItYla5NyMNR"
      },
      "outputs": [],
      "source": [
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "if outputs.shape == batch_emb.shape:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🐙 다시 도전해봐요!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
